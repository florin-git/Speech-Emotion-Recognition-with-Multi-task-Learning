{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ccc89cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    trainer_utils,\n",
    ")\n",
    "from model import Wav2Vec2ForCTCnCLS\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional, Set, Union\n",
    "from arguments import ModelArguments, DataTrainingArguments\n",
    "\n",
    "import yaml\n",
    "import os\n",
    "import random\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f66b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "SEED = 10\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3a6050",
   "metadata": {},
   "outputs": [],
   "source": [
    "yaml_file = \"reproduce.yaml\"\n",
    "# yaml_file = \"train.yaml\"\n",
    "# yaml_file = \"predict.yaml\"\n",
    "\n",
    "# Read configuration file with all the necessary parameters\n",
    "with open(yaml_file) as file:\n",
    "    config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfba2d48",
   "metadata": {},
   "source": [
    "Retrieve arguments from the respective yaml file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bda4496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "model_args = ModelArguments(**config['model_args'])\n",
    "data_args = DataTrainingArguments(**config['data_args'])\n",
    "training_args = TrainingArguments(**config['training_args'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a2f09",
   "metadata": {},
   "source": [
    "Load preprocessed datasets and processor from disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7440334f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT_ID identifies the test split\n",
    "SPLIT_ID = data_args.split_id \n",
    "\n",
    "if data_args.load_reduced:\n",
    "    train_path = f\"./datasets/reduced_train_ds/{SPLIT_ID}\"\n",
    "    val_path = f\"./datasets/reduced_val_ds/{SPLIT_ID}\"\n",
    "    \n",
    "else:\n",
    "    train_path = f\"./datasets/prepared_train_ds/{SPLIT_ID}\"\n",
    "    val_path = f\"./datasets/prepared_val_ds/{SPLIT_ID}\"\n",
    "\n",
    "prepared_train_dataset = datasets.load_from_disk(train_path)\n",
    "prepared_val_dataset = datasets.load_from_disk(val_path)\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"datasets/processor\")\n",
    "\n",
    "cls_label_map = {\"e0\":0, \"e1\":1, \"e2\":2, \"e3\":3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac5d53",
   "metadata": {},
   "source": [
    "Emotion Classes:\n",
    "- e0: Neutral\n",
    "- e1: Happy\n",
    "- e2: Angry\n",
    "- e3: Sad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58d385e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for processing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "    audio_only = False\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # Split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        \n",
    "        if self.audio_only is False: # Training\n",
    "            label_features = [{\"input_ids\": feature[\"labels\"][:-1]} for feature in features]\n",
    "            # Class label is the last element in the feature list\n",
    "            cls_labels = [feature[\"labels\"][-1] for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        if self.audio_only is False:\n",
    "            with self.processor.as_target_processor():\n",
    "                labels_batch = self.processor.pad(\n",
    "                    label_features,\n",
    "                    padding=self.padding,\n",
    "                    max_length=self.max_length_labels,\n",
    "                    pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "\n",
    "            # Replace padding with -100 to ignore loss correctly\n",
    "            ctc_labels = labels_batch[\"input_ids\"].masked_fill_(labels_batch.attention_mask.ne(1), -100)\n",
    "            batch[\"labels\"] = (ctc_labels, torch.tensor(cls_labels)) # labels = (ctc_labels, cls_labels)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa0317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)\n",
    "wer_metric = datasets.load_metric(\"wer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a299b1a",
   "metadata": {},
   "source": [
    "**Word error rate** (**WER**) is used as performance metric for the speech recognition task.\n",
    "\n",
    "It consists in measuring the difference, in terms of character edits (insertions, deletions or substitutions), between two sequences, i.e., the predicted text and the actual text.\n",
    "\n",
    "$$\n",
    "{\\displaystyle {\\mathit {WER}}={\\frac {S+D+I}{N}}={\\frac {S+D+I}{S+D+C}}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- *S* is the number of substitutions,\n",
    "- *D* is the number of deletions, \n",
    "- *I* is the number of insertions,\n",
    "- *C* is the number of correct words,\n",
    "- *N* is the number of words in the reference (N=S+D+C)\n",
    "\n",
    "The **lower** the value, the **better** the performance of the ASR system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14dc9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    # Emotion labels predictions\n",
    "    cls_pred_logits = pred.predictions[1]\n",
    "    cls_pred_ids = np.argmax(cls_pred_logits, axis=-1)\n",
    "    total = len(pred.label_ids[1])\n",
    "    correct = (cls_pred_ids == pred.label_ids[1]).sum().item() # label = (ctc_label, cls_label)\n",
    "\n",
    "    # Transcript predictions\n",
    "    ctc_pred_logits = pred.predictions[0]\n",
    "    ctc_pred_ids = np.argmax(ctc_pred_logits, axis=-1)\n",
    "    pred.label_ids[0][pred.label_ids[0] == -100] = processor.tokenizer.pad_token_id\n",
    "\n",
    "    # WER operates on string, hence we have to decode the ids\n",
    "    ctc_pred_str = processor.batch_decode(ctc_pred_ids)\n",
    "    # We do not want to group tokens when computing the metrics\n",
    "    ctc_label_str = processor.batch_decode(pred.label_ids[0], group_tokens=False)\n",
    "    \n",
    "    wer = wer_metric.compute(predictions=ctc_pred_str, references=ctc_label_str)\n",
    "    return {\"acc\": correct/total, \"wer\": wer, \"correct\": correct, \"total\": total, \"strlen\": len(ctc_label_str)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999e9371",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be944f2",
   "metadata": {},
   "source": [
    "![image](images/architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636cd35",
   "metadata": {},
   "source": [
    "The <span style=\"color:orange\">**orange**</span> part correponds to the ASR (**A**utomatic **S**peech **R**ecognition) task: it **inputs** speech features and **outputs** text.\n",
    "\n",
    "The <span style=\"color:blue\">**blue**</span> part corresponds to the SER (**S**peech **E**motion **R**ecognition) task: it **inputs** speech features and **outputs** emotion labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b54c05",
   "metadata": {},
   "source": [
    "The **loss** is composed by two losses:\n",
    "- The CTC loss for the transcriptions:\n",
    "    - $\\mathcal{L}_{CTC} = CTC(\\hat{y}, t)$, where $\\hat{y}$ are the text predictions.\n",
    "- The Cross-entropy loss for the emotion labels:\n",
    "    - $\\mathcal{L}_{CE} = CrossEntropy(\\hat{c}, t)$, where $\\hat{c}$ are the predicted labels\n",
    "\n",
    "The $\\alpha$ hyper-parameter combines the two losses and controls the relative importance of the CTC loss of the ASR task.\n",
    "\n",
    "$\\mathcal{L} = \\mathcal{L}_{CE} + \\alpha\\mathcal{L}_{CTC}$\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f6c3506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ckpts/02F were not used when initializing Wav2Vec2ForCTCnCLS: ['vib_ctc.emb2mu.bias', 'vib_ctc.std_p', 'vib_cls.emb2std.bias', 'vib_cls.emb2std.weight', 'vib_cls.classifier.bias', 'vib_cls.mu_p', 'vib_ctc.emb2std.bias', 'vib_cls.mlp.0.bias', 'vib_ctc.classifier.bias', 'vib_cls.mlp.2.weight', 'vib_ctc.emb2std.weight', 'vib_cls.classifier.weight', 'vib_ctc.mlp.0.bias', 'vib_cls.emb2mu.bias', 'vib_ctc.mlp.2.bias', 'vib_ctc.mlp.0.weight', 'vib_ctc.mlp.2.weight', 'vib_cls.emb2mu.weight', 'vib_ctc.classifier.weight', 'vib_ctc.mu_p', 'vib_cls.mlp.0.weight', 'vib_cls.mlp.2.bias', 'vib_ctc.emb2mu.weight', 'vib_cls.std_p']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTCnCLS from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTCnCLS from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = Wav2Vec2ForCTCnCLS.from_pretrained(\n",
    "        model_args.model_name_or_path,\n",
    "        cache_dir=model_args.cache_dir,\n",
    "        gradient_checkpointing=training_args.gradient_checkpointing,\n",
    "        vocab_size=len(processor.tokenizer),\n",
    "        cls_len=len(cls_label_map),\n",
    "        alpha=model_args.alpha,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09fec9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCTrainer(Trainer):\n",
    "    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n",
    "        for k, v in inputs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                kwargs = dict(device=self.args.device)\n",
    "                if self.deepspeed and inputs[k].dtype != torch.int64:\n",
    "                    kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype()))\n",
    "                inputs[k] = v.to(**kwargs)\n",
    "\n",
    "            if k == 'labels': # labels are list of tensor, not tensor, special handle here\n",
    "                for i in range(len(inputs[k])):\n",
    "                    kwargs = dict(device=self.args.device)\n",
    "                    if self.deepspeed and inputs[k][i].dtype != torch.int64:\n",
    "                        kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype()))\n",
    "                    inputs[k][i] = inputs[k][i].to(**kwargs)\n",
    "\n",
    "        if self.args.past_index >= 0 and self._past is not None:\n",
    "            inputs[\"mems\"] = self._past\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "        \n",
    "        if self.use_amp:\n",
    "            if \"cuda\" in self.args.device.type:\n",
    "                with torch.autocast(\"cuda\"):\n",
    "                    loss = self.compute_loss(model, inputs)\n",
    "            else:\n",
    "                with torch.autocast(\"cpu\"):\n",
    "                    loss = self.compute_loss(model, inputs)\n",
    "        else:\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.use_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6398a0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp half precision backend\n"
     ]
    }
   ],
   "source": [
    "trainer = CTCTrainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=prepared_train_dataset,\n",
    "        eval_dataset=prepared_val_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbdfe16c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make sure that the learning rate is read as a number and not as a string\n",
    "training_args.learning_rate = float(training_args.learning_rate)\n",
    "training_args.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5040a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detecting last checkpoint\n",
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None:\n",
    "        print(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f002b93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "elif model_args.model_name_or_path is not None and os.path.isdir(model_args.model_name_or_path):\n",
    "    checkpoint = model_args.model_name_or_path\n",
    "else:\n",
    "    checkpoint = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be14febb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    trainer.train(resume_from_checkpoint=checkpoint)\n",
    "    trainer.save_model() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb277798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_predictions(dataset: datasets.arrow_dataset.Dataset, pred_ids: np.ndarray, pred_probs: torch.tensor):\n",
    "    # Write predictions to a file\n",
    "    with open(data_args.output_file, 'w') as f:\n",
    "        f.write(\"Wav File\\t Duration\\t Emotion Class\\t Probabilities\\n\")\n",
    "        for i in range(len(pred_ids)):\n",
    "            wav_file = dataset[i]['file'].split(\"/\")[-1]\n",
    "            duration = str(len(dataset[i]['input_values'])/16000)\n",
    "            f.write(wav_file+ \" \" + duration + \" \")\n",
    "            pred = pred_ids[i]\n",
    "            f.write(str(pred)+' ')\n",
    "            for j in range(4):\n",
    "                f.write(' ' + str(pred_probs[i][j].item()))\n",
    "            f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7c560bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 505\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='505' max='505' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [505/505 01:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: 418 , acc: 0.8277227722772277\n"
     ]
    }
   ],
   "source": [
    "if training_args.do_predict:\n",
    "    print('******* Predict ********')\n",
    "    data_collator.audio_only=True\n",
    "    # In predictions we do not have the labels \n",
    "    predictions, _ , metrics = trainer.predict(prepared_val_dataset, metric_key_prefix=\"predict\")\n",
    "    logits_ctc, logits_cls = predictions\n",
    "    pred_ids = np.argmax(logits_cls, axis=-1)\n",
    "    pred_probs = F.softmax(torch.from_numpy(logits_cls).float(), dim=-1)\n",
    "\n",
    "    # Write predictions to a file\n",
    "    if 'file' in prepared_val_dataset.column_names:\n",
    "        write_predictions(prepared_val_dataset, pred_ids, pred_probs)\n",
    "    \n",
    "\n",
    "elif training_args.do_eval:\n",
    "    predictions, labels, metrics = trainer.predict(prepared_val_dataset, metric_key_prefix=\"eval\")\n",
    "    logits_ctc, logits_cls = predictions\n",
    "    pred_ids = np.argmax(logits_cls, axis=-1)\n",
    "    correct = np.sum(pred_ids == labels[1])\n",
    "    acc = correct / len(pred_ids)\n",
    "    print('correct:', correct, ', acc:', acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4896358c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf37a2529baf03c803266b8d55d553bda8f92a60c47b8d9f7bdcfc02cbd55fef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
