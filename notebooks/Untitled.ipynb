{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "264ea5f8",
   "metadata": {},
   "source": [
    "https://huggingface.co/blog/fine-tune-wav2vec2-english\n",
    "\n",
    "https://github.com/Demfier/multimodal-speech-emotion-recognition/blob/master/2_build_audio_vectors.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ccc89cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pathlib\n",
    "import re\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Callable, Dict, List, Optional, Set, Union\n",
    "\n",
    "import datasets\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from packaging import version\n",
    "\n",
    "import librosa\n",
    "from lang_trans import arabic\n",
    "\n",
    "import soundfile as sf\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f66b46d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    Wav2Vec2CTCTokenizer,\n",
    "    Wav2Vec2FeatureExtractor,\n",
    "    Wav2Vec2Processor,\n",
    "    is_apex_available,\n",
    "    trainer_utils,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c313abef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Read configuration file with all the necessary parameters\n",
    "with open('conf.yaml') as file:\n",
    "  config = yaml.safe_load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "020631a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Orthography:\n",
    "    \"\"\"\n",
    "    Orthography scheme used for text normalization and tokenization.\n",
    "\n",
    "    Args:\n",
    "        do_lower_case (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Whether or not to accept lowercase input and lowercase the output when decoding.\n",
    "        vocab_file (:obj:`str`, `optional`, defaults to :obj:`None`):\n",
    "            File containing the vocabulary.\n",
    "        word_delimiter_token (:obj:`str`, `optional`, defaults to :obj:`\"|\"`):\n",
    "            The token used for delimiting words; it needs to be in the vocabulary.\n",
    "        translation_table (:obj:`Dict[str, str]`, `optional`, defaults to :obj:`{}`):\n",
    "            Table to use with `str.translate()` when preprocessing text (e.g., \"-\" -> \" \").\n",
    "        words_to_remove (:obj:`Set[str]`, `optional`, defaults to :obj:`set()`):\n",
    "            Words to remove when preprocessing text (e.g., \"sil\").\n",
    "        untransliterator (:obj:`Callable[[str], str]`, `optional`, defaults to :obj:`None`):\n",
    "            Function that untransliterates text back into native writing system.\n",
    "    \"\"\"\n",
    "\n",
    "    do_lower_case: bool = False\n",
    "    vocab_file: Optional[str] = None\n",
    "    word_delimiter_token: Optional[str] = \"|\"\n",
    "    translation_table: Optional[Dict[str, str]] = field(default_factory=dict)\n",
    "    words_to_remove: Optional[Set[str]] = field(default_factory=set)\n",
    "    untransliterator: Optional[Callable[[str], str]] = None\n",
    "    tokenizer: Optional[str] = None\n",
    "\n",
    "    @classmethod\n",
    "    def from_name(cls, name: str):\n",
    "        if name == \"librispeech\":\n",
    "            return cls()\n",
    "        if name == \"timit\":\n",
    "            return cls(\n",
    "                do_lower_case=True,\n",
    "                # break compounds like \"quarter-century-old\" and replace pauses \"--\"\n",
    "                translation_table=str.maketrans({\"-\": \" \"}),\n",
    "            )\n",
    "        if name == \"buckwalter\":\n",
    "            translation_table = {\n",
    "                \"-\": \" \",  # sometimes used to represent pauses\n",
    "                \"^\": \"v\",  # fixing \"tha\" in arabic_speech_corpus dataset\n",
    "            }\n",
    "            return cls(\n",
    "                vocab_file=pathlib.Path(__file__).parent.joinpath(\"vocab/buckwalter.json\"),\n",
    "                word_delimiter_token=\"/\",  # \"|\" is Arabic letter alef with madda above\n",
    "                translation_table=str.maketrans(translation_table),\n",
    "                words_to_remove={\"sil\"},  # fixing \"sil\" in arabic_speech_corpus dataset\n",
    "                untransliterator=arabic.buckwalter.untransliterate,\n",
    "            )\n",
    "        raise ValueError(f\"Unsupported orthography: '{name}'.\")\n",
    "\n",
    "    def preprocess_for_training(self, text: str) -> str:\n",
    "        if len(self.translation_table) > 0:\n",
    "            text = text.translate(self.translation_table)\n",
    "        \n",
    "        if len(self.words_to_remove) == 0:\n",
    "            try:\n",
    "                text = \" \".join(text.split())  # clean up whitespaces\n",
    "            except Exception:\n",
    "                text = \"NULL\"\n",
    "        else:\n",
    "            text = \" \".join(w for w in text.split() if w not in self.words_to_remove)  # and clean up whilespaces\n",
    "        return text\n",
    "\n",
    "    def create_processor(self, model_args: dict) -> Wav2Vec2Processor:\n",
    "        feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(\n",
    "                                model_args[\"name\"], \n",
    "                                cache_dir=model_args['cache_dir']\n",
    "                            )\n",
    "        if self.vocab_file:\n",
    "            tokenizer = Wav2Vec2CTCTokenizer(\n",
    "                self.vocab_file,\n",
    "                cache_dir=model_args['cache_dir'],\n",
    "                do_lower_case=self.do_lower_case,\n",
    "                word_delimiter_token=self.word_delimiter_token,\n",
    "            )\n",
    "        else:\n",
    "            tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(\n",
    "                self.tokenizer,\n",
    "                cache_dir=model_args['cache_dir'],\n",
    "                do_lower_case=self.do_lower_case,\n",
    "                word_delimiter_token=self.word_delimiter_token,\n",
    "            )\n",
    "        return Wav2Vec2Processor(feature_extractor, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75eb669e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\transformers\\configuration_utils.py:364: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\n"
     ]
    }
   ],
   "source": [
    "orthography = Orthography.from_name(config['dataset']['orthography'])\n",
    "orthography.tokenizer = config['model']['tokenizer']\n",
    "\n",
    "processor = orthography.create_processor(config['model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5feb0ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2Processor:\n",
       "- feature_extractor: Wav2Vec2FeatureExtractor {\n",
       "  \"do_normalize\": true,\n",
       "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
       "  \"feature_size\": 1,\n",
       "  \"padding_side\": \"right\",\n",
       "  \"padding_value\": 0.0,\n",
       "  \"return_attention_mask\": false,\n",
       "  \"sampling_rate\": 16000\n",
       "}\n",
       "\n",
       "- tokenizer: PreTrainedTokenizer(name_or_path='facebook/wav2vec2-base', vocab_size=32, model_max_len=1000000000000000019884624838656, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b16f3ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_IEMOCAP_FOLDER = config['dataset']['folder_path']\n",
    "SPLIT_ID = config['dataset']['split_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f21ba4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df_train = pd.read_csv(f'{INPUT_IEMOCAP_FOLDER}/iemocap_{SPLIT_ID}.train.csv')\n",
    "df_val = pd.read_csv(f'{INPUT_IEMOCAP_FOLDER}/iemocap_{SPLIT_ID}.test.csv')\n",
    "\n",
    "# Change file location to previous folder '../'\n",
    "df_train['file'] = df_train['file'].map(lambda x: f\"../{x}\")\n",
    "df_val['file'] = df_val['file'].map(lambda x: f\"../{x}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d3e83955",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(df_train)\n",
    "val_dataset = Dataset.from_pandas(df_val)\n",
    "cls_label_map = {\"e0\":0, \"e1\":1, \"e2\":2, \"e3\":3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7248632e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/facebook/wav2vec2-base/resolve/main/config.json from cache at cache\\c7746642f045322fd01afa31271dd490e677ea11999e68660a92619ec7c892b4.ce1f96bfaf3d7475cb8187b9668c7f19437ade45fb9ceb78d2b06a2cec198015\n",
      "Model config Wav2Vec2Config {\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 256,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": false,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"sum\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": false,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_norm\": \"group\",\n",
      "  \"feat_proj_dropout\": 0.1,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"freeze_feat_extract_train\": true,\n",
      "  \"gradient_checkpointing\": true,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.0,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"no_mask_channel_overlap\": false,\n",
      "  \"no_mask_time_overlap\": false,\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 768,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"proj_codevector_dim\": 256,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"transformers_version\": \"4.19.1\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 32,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/facebook/wav2vec2-base/resolve/main/pytorch_model.bin from cache at cache\\ef45231897ce572a660ebc5a63d3702f1a6041c4c5fb78cbec330708531939b3.fcae05302a685f7904c551c8ea571e8bc2a2c4a1777ea81ad66e47f7883a650a\n",
      "Some weights of the model checkpoint at facebook/wav2vec2-base were not used when initializing Wav2Vec2ForCTCnCLS: ['quantizer.weight_proj.bias', 'quantizer.codevectors', 'project_q.bias', 'quantizer.weight_proj.weight', 'project_hid.weight', 'project_hid.bias', 'project_q.weight']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTCnCLS from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTCnCLS from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTCnCLS were not initialized from the model checkpoint at facebook/wav2vec2-base and are newly initialized: ['cls_head.bias', 'lm_head.bias', 'lm_head.weight', 'cls_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from model import Wav2Vec2ForCTCnCLS\n",
    "\n",
    "model = Wav2Vec2ForCTCnCLS.from_pretrained(\n",
    "    config['model']['name'],\n",
    "    cache_dir=config['model']['cache_dir'],\n",
    "    gradient_checkpointing=config['training']['gradient_checkpointing'],\n",
    "    vocab_size=len(processor.tokenizer),\n",
    "    cls_len=len(cls_label_map),\n",
    "    alpha=config['model']['alpha'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ceb2ed1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "wer_metric = datasets.load_metric(\"wer\")\n",
    "\n",
    "target_sr = processor.feature_extractor.sampling_rate #if config['training']['target_feature_extractor_sampling_rate'] else None\n",
    "\n",
    "vocabulary_chars_str = \"\".join(t for t in processor.tokenizer.get_vocab().keys() if len(t) == 1)\n",
    "vocabulary_text_cleaner = re.compile(  # remove characters not in vocabulary\n",
    "    f\"[^\\s{re.escape(vocabulary_chars_str)}]\",  # allow space in addition to chars in vocabulary\n",
    "    flags=re.IGNORECASE if processor.tokenizer.do_lower_case else 0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9daec409",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['file', 'emotion', 'text'],\n",
       "    num_rows: 5024\n",
       "})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "d102b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_updates = []\n",
    "\n",
    "def prepare_sample(sample, audio_only=False):\n",
    "    sample[\"speech\"], sample[\"sampling_rate\"] = librosa.load(sample[config['dataset']['speech_file_column']], \n",
    "                                                                    sr=target_sr)\n",
    "    \n",
    "    if audio_only is False:\n",
    "        # Normalize and clean up text; order matters!\n",
    "        updated_text = orthography.preprocess_for_training(sample[config['dataset']['target_text_column']])\n",
    "        updated_text = vocabulary_text_cleaner.sub(\"\", updated_text)\n",
    "        if updated_text != sample[config['dataset']['target_text_column']]:\n",
    "            text_updates.append((sample[config['dataset']['target_text_column']], updated_text))\n",
    "            sample[config['dataset']['target_text_column']] = updated_text\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "fa2b61b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5024/5024 [01:32<00:00, 54.58ex/s]\n",
      "100%|██████████| 507/507 [00:12<00:00, 41.24ex/s]\n"
     ]
    }
   ],
   "source": [
    "if config['training']['do_train']:\n",
    "    train_dataset = train_dataset.map(prepare_sample, remove_columns=[config['dataset']['speech_file_column']])\n",
    "\n",
    "if config['training']['do_eval']:\n",
    "    val_dataset = val_dataset.map(prepare_sample, remove_columns=[config['dataset']['speech_file_column']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "450cf846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InMemoryTable\n",
       "emotion: string\n",
       "text: string\n",
       "speech: list<item: float>\n",
       "  child 0, item: float\n",
       "sampling_rate: int64\n",
       "----\n",
       "emotion: [[\"e0\",\"e0\",\"e0\",\"e0\",\"e2\",\"e0\",\"e2\",\"e2\",\"e3\",\"e3\"]]\n",
       "text: [[\"EXCUSE ME \",\"YEAH \",\"IS THERE A PROBLEM \",\"WELL WHAT'S THE PROBLEM LET ME CHANGE IT \",\"THAT'S OUT OF CONTROL \",\"CLEARLY  YOU KNOW DO YOU HAVE LIKE A SUPERVISOR OR SOMETHING \",\"I DON'T UNDERSTAND WHY THIS IS SO COMPLICATED FOR PEOPLE WHEN THEY GET HERE  IT'S JUST A SIMPLE FORM I JUST NEED AN ID \",\"YEAH DO YOU WANT  TO SEE MY SUPERVISOR HUH YEAH DO YOU WANT TO SEE MY SUPERVISOR FINE I'LL BE RIGHT BACK \",\"DID YOU GET THE MAIL SO YOU SAW MY LETTER \",\"YEAH I KNOW \"]]\n",
       "speech: [[[-0.0050354004,-0.0049743652,-0.0038146973,-0.0032653809,-0.0025939941,...,-0.0033569336,-0.0030212402,-0.0026550293,-0.0031738281,-0.004180908],[0.0009460449,-0.0009460449,-0.0007019043,-0.0005493164,-0.0021972656,...,-0.0010986328,-0.0011901855,-0.00045776367,-0.00033569336,-0.0012817383],...,[0.0007324219,0.00048828125,0.00061035156,0.0020751953,0.0025024414,...,-0.001739502,-0.00036621094,0.0012512207,0.0021972656,0.0027160645],[-0.0016174316,-0.0016479492,-0.001953125,-0.0012817383,-0.00076293945,...,-0.0018005371,-0.0017700195,-0.0015563965,-0.0014038086,-0.0015258789]]]\n",
       "sampling_rate: [[16000,16000,16000,16000,16000,16000,16000,16000,16000,16000]]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset._data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37912d21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [9, 5, 28, 6, 4], 'attention_mask': [1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with processor.as_target_processor():\n",
    "    a = processor(train_dataset[0]['text'])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "597d485e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_dataset(batch, processor, cls_label_map: dict, \n",
    "                    config: dict, audio_only=False):\n",
    "\n",
    "    # Check that all files have the correct sampling rate\n",
    "    assert (\n",
    "        len(set(batch[\"sampling_rate\"])) == 1\n",
    "    ), f\"Make sure all inputs have the same sampling rate of {processor.feature_extractor.sampling_rate}.\"\n",
    "    \n",
    "    batch[\"input_values\"] = processor(batch[\"speech\"], sampling_rate=batch[\"sampling_rate\"][0]).input_values\n",
    "    if audio_only is False:\n",
    "        # Map labels to their respective integers\n",
    "        cls_labels = list(map(lambda e: cls_label_map[e], batch[\"emotion\"]))\n",
    "        \n",
    "        with processor.as_target_processor():\n",
    "            batch[\"labels\"] = processor(batch[config['dataset']['target_text_column']]).input_ids\n",
    "        for i in range(len(cls_labels)):\n",
    "            batch[\"labels\"][i].append(cls_labels[i]) # batch[\"labels\"] element has to be a single list\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "1d452775",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function prepare_dataset at 0x00000240B8A21AF8> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "  0%|          | 0/2512 [00:00<?, ?ba/s]c:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\transformers\\feature_extraction_utils.py:168: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tensor = as_tensor(value)\n",
      " 36%|███▋      | 914/2512 [01:21<02:22, 11.20ba/s]\n"
     ]
    },
    {
     "ename": "ArrowMemoryError",
     "evalue": "realloc of size 2147483648 failed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mArrowMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_28884\\1619144628.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'per_device_train_batch_size'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                         \u001b[0mbatched\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                         \u001b[0mnum_proc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'training'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'preprocessing_num_workers'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m                     )\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[0;32m   2362\u001b[0m                 \u001b[0mnew_fingerprint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_fingerprint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2363\u001b[0m                 \u001b[0mdisable_tqdm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable_tqdm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2364\u001b[1;33m                 \u001b[0mdesc\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdesc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2365\u001b[0m             )\n\u001b[0;32m   2366\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m\"Dataset\"\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"self\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdataset\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    497\u001b[0m         }\n\u001b[0;32m    498\u001b[0m         \u001b[1;31m# apply actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[0mout\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"DatasetDict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m         \u001b[0mdatasets\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Dataset\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;31m# re-apply format to the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\fingerprint.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# Call actual function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m             \u001b[1;31m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\arrow_dataset.py\u001b[0m in \u001b[0;36m_map_single\u001b[1;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset, disable_tqdm, desc, cache_only)\u001b[0m\n\u001b[0;32m   2749\u001b[0m                                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2750\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2751\u001b[1;33m                                 \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2752\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mupdate_data\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mwriter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2753\u001b[0m                     \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# close_stream=bool(buf_writer is None))  # We only close if we are writing in a file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\arrow_writer.py\u001b[0m in \u001b[0;36mwrite_batch\u001b[1;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[0;32m    509\u001b[0m         \u001b[0mschema\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minferred_features\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marrow_schema\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpa_writer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m         \u001b[0mpa_table\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_arrays\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpa_table\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    512\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrite_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpa\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwriter_batch_size\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\datasets\\arrow_writer.py\u001b[0m in \u001b[0;36mwrite_table\u001b[1;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[0;32m    526\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_examples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mpa_table\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mbatches\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpa_writer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    529\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclose_stream\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\pyarrow\\ipc.pxi\u001b[0m in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_batch\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\pyarrow\\error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mArrowMemoryError\u001b[0m: realloc of size 2147483648 failed"
     ]
    }
   ],
   "source": [
    "if config['training']['do_train']:\n",
    "    prepared_train_dataset = train_dataset.map(\n",
    "                        prepare_dataset,\n",
    "                        fn_kwargs={'processor': processor, 'cls_label_map': cls_label_map,\n",
    "                                   'config': config},\n",
    "                        batch_size=config['training']['per_device_train_batch_size'],\n",
    "                        batched=True,\n",
    "                        num_proc=config['training']['preprocessing_num_workers'],\n",
    "                    )\n",
    "\n",
    "if config['training']['do_eval']:\n",
    "    prepared_val_dataset = val_dataset.map(\n",
    "                        prepare_dataset,\n",
    "                        fn_kwargs={'processor': processor, 'cls_label_map': cls_label_map,\n",
    "                                   'config': config},\n",
    "                        batch_size=config['training']['per_device_train_batch_size'],\n",
    "                        batched=True,\n",
    "                        num_proc=config['training']['preprocessing_num_workers'],\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8d8bf9da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['emotion', 'text', 'speech', 'sampling_rate', 'input_values', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prepared_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "58d385e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorCTCWithPadding:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs received.\n",
    "    Args:\n",
    "        processor (:class:`~transformers.Wav2Vec2Processor`)\n",
    "            The processor used for processing the data.\n",
    "        padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n",
    "            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n",
    "            among:\n",
    "            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
    "              sequence if provided).\n",
    "            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n",
    "              maximum acceptable input length for the model if that argument is not provided.\n",
    "            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n",
    "              different lengths).\n",
    "        max_length (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n",
    "        max_length_labels (:obj:`int`, `optional`):\n",
    "            Maximum length of the ``labels`` returned list and optionally padding length (see above).\n",
    "        pad_to_multiple_of (:obj:`int`, `optional`):\n",
    "            If set will pad the sequence to a multiple of the provided value.\n",
    "            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n",
    "            7.5 (Volta).\n",
    "    \"\"\"\n",
    "\n",
    "    processor: Wav2Vec2Processor\n",
    "    padding: Union[bool, str] = True\n",
    "    max_length: Optional[int] = None\n",
    "    max_length_labels: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    pad_to_multiple_of_labels: Optional[int] = None\n",
    "    audio_only = False\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        # split inputs and labels since they have to be of different lenghts and need\n",
    "        # different padding methods\n",
    "        input_features = [{\"input_values\": feature[\"input_values\"]} for feature in features]\n",
    "        \n",
    "        if self.audio_only is False: # Training\n",
    "            label_features = [{\"input_ids\": feature[\"labels\"][:-1]} for feature in features]\n",
    "            cls_labels = [feature[\"labels\"][-1] for feature in features]\n",
    "\n",
    "        batch = self.processor.pad(\n",
    "            input_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        if self.audio_only is False:\n",
    "            with self.processor.as_target_processor():\n",
    "                labels_batch = self.processor.pad(\n",
    "                    label_features,\n",
    "                    padding=self.padding,\n",
    "                    max_length=self.max_length_labels,\n",
    "                    pad_to_multiple_of=self.pad_to_multiple_of_labels,\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "\n",
    "            # Replace padding with -100 to ignore loss correctly\n",
    "            ctc_labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
    "            batch[\"labels\"] = (ctc_labels, torch.tensor(cls_labels)) # labels = (ctc_labels, cls_labels)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8aa0317f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorCTCWithPadding(processor=processor, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "14dc9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    cls_pred_logits = pred.predictions[1]\n",
    "    cls_pred_ids = np.argmax(cls_pred_logits, axis=-1)\n",
    "    total = len(pred.label_ids[1])\n",
    "    correct = (cls_pred_ids == pred.label_ids[1]).sum().item() # label = (ctc_label, cls_label)\n",
    "\n",
    "    ctc_pred_logits = pred.predictions[0]\n",
    "    ctc_pred_ids = np.argmax(ctc_pred_logits, axis=-1)\n",
    "    pred.label_ids[0][pred.label_ids[0] == -100] = processor.tokenizer.pad_token_id\n",
    "    ctc_pred_str = processor.batch_decode(ctc_pred_ids)\n",
    "    # we do not want to group tokens when computing the metrics\n",
    "    ctc_label_str = processor.batch_decode(pred.label_ids[0], group_tokens=False)\n",
    "    \n",
    "    wer = wer_metric.compute(predictions=ctc_pred_str, references=ctc_label_str)\n",
    "    return {\"acc\": correct/total, \"wer\": wer, \"correct\": correct, \"total\": total, \"strlen\": len(ctc_label_str)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "09fec9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCTrainer(Trainer):\n",
    "    def _prepare_inputs(self, inputs: Dict[str, Union[torch.Tensor, Any]]) -> Dict[str, Union[torch.Tensor, Any]]:\n",
    "        for k, v in inputs.items():\n",
    "            if isinstance(v, torch.Tensor):\n",
    "                kwargs = dict(device=self.args.device)\n",
    "                # if self.deepspeed and inputs[k].dtype != torch.int64:\n",
    "                #     kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype()))\n",
    "                inputs[k] = v.to(**kwargs)\n",
    "\n",
    "            if k == 'labels': # labels are list of tensor, not tensor, special handle here\n",
    "                for i in range(len(inputs[k])):\n",
    "                    kwargs = dict(device=self.args.device)\n",
    "                    # if self.deepspeed and inputs[k][i].dtype != torch.int64:\n",
    "                    #     kwargs.update(dict(dtype=self.args.hf_deepspeed_config.dtype()))\n",
    "                    inputs[k][i] = inputs[k][i].to(**kwargs)\n",
    "\n",
    "        if self.args.past_index >= 0 and self._past is not None:\n",
    "            inputs[\"mems\"] = self._past\n",
    "\n",
    "        return inputs\n",
    "\n",
    "    def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor, Any]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Perform a training step on a batch of inputs.\n",
    "\n",
    "        Subclass and override to inject custom behavior.\n",
    "\n",
    "        Args:\n",
    "            model (:obj:`nn.Module`):\n",
    "                The model to train.\n",
    "            inputs (:obj:`Dict[str, Union[torch.Tensor, Any]]`):\n",
    "                The inputs and targets of the model.\n",
    "\n",
    "                The dictionary will be unpacked before being fed to the model. Most models expect the targets under the\n",
    "                argument :obj:`labels`. Check your model's documentation for all accepted arguments.\n",
    "\n",
    "        Return:\n",
    "            :obj:`torch.Tensor`: The tensor with training loss on this batch.\n",
    "        \"\"\"\n",
    "\n",
    "        model.train()\n",
    "        inputs = self._prepare_inputs(inputs)\n",
    "\n",
    "        if self.use_amp:\n",
    "            with autocast():\n",
    "                loss = self.compute_loss(model, inputs)\n",
    "        else:\n",
    "            loss = self.compute_loss(model, inputs)\n",
    "\n",
    "        if self.args.n_gpu > 1:\n",
    "            loss = loss.mean()\n",
    "\n",
    "        if self.args.gradient_accumulation_steps > 1:\n",
    "            loss = loss / self.args.gradient_accumulation_steps\n",
    "\n",
    "        if self.use_amp:\n",
    "            self.scaler.scale(loss).backward()\n",
    "        elif self.use_apex:\n",
    "            with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        elif self.deepspeed:\n",
    "            self.deepspeed.backward(loss)\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        return loss.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6398a0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No `TrainingArguments` passed, using `output_dir=tmp_trainer`.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "trainer = CTCTrainer(\n",
    "        model=model,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=prepared_train_dataset,\n",
    "        eval_dataset=prepared_val_dataset,\n",
    "        tokenizer=processor.feature_extractor,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be14febb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTCnCLS.forward` and have been ignored: sampling_rate, text, emotion, speech. If sampling_rate, text, emotion, speech are not expected by `Wav2Vec2ForCTCnCLS.forward`,  you can safely ignore this message.\n",
      "c:\\Users\\flori\\miniconda3\\envs\\DL\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 6\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "100%|██████████| 6/6 [00:08<00:00,  1.22s/it]\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 6/6 [00:08<00:00,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 8.3723, 'train_samples_per_second': 3.583, 'train_steps_per_second': 0.717, 'train_loss': 562.1226399739584, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=562.1226399739584, metrics={'train_runtime': 8.3723, 'train_samples_per_second': 3.583, 'train_steps_per_second': 0.717, 'train_loss': 562.1226399739584, 'epoch': 3.0})"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('DL')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "bf37a2529baf03c803266b8d55d553bda8f92a60c47b8d9f7bdcfc02cbd55fef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
